{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e6b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import orgelpredigt_analysis as oa\n",
    "from rapidfuzz import fuzz\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "import io\n",
    "import datetime\n",
    "from numpyencoder import NumpyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66360441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(a, b, threshold=80):\n",
    "    similarity_score = fuzz.ratio(a, b)\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546979a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_song_in_book(id):\n",
    "    match = re.findall(r'E10[0-9]{4}', id)[0]\n",
    "    \n",
    "    with open('songs_to_pages_mapping.json') as f:\n",
    "        songbook_pages = json.load(f)\n",
    "    if songbook_pages[match][\"pages\"] == '':\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f5f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_page(id): \n",
    "    match = re.findall(r'E10[0-9]{4}', id)[0]\n",
    "    with open('songs_to_pages_mapping.json') as f:\n",
    "        songbook_pages = json.load(f)\n",
    "    page = songbook_pages[match][\"pages\"]\n",
    "    \n",
    "    return [int(page) + 42, int(page) + 43, int(page) + 44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f58df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9373e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_consecutive(L):\n",
    "    return all(n-i==L[0] for i,n in enumerate(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7ff83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_equal(L):\n",
    "    return all(n == L[0] for n in L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585b5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_proxy(numbers):\n",
    "  \"\"\"\n",
    "  Checks if a list of numbers are either all the same or have a maximum difference of 1 between any two numbers.\n",
    "\n",
    "  Args:\n",
    "    numbers: A list of numbers.\n",
    "\n",
    "  Returns:\n",
    "    True if the numbers meet the criteria, False otherwise.  Returns False if the list is empty.\n",
    "  \"\"\"\n",
    "\n",
    "  if not numbers:\n",
    "    return False  # Handle empty list case\n",
    "\n",
    "  first_number = numbers[0]\n",
    "  all_same = True\n",
    "  max_diff_one = True\n",
    "\n",
    "  for number in numbers:\n",
    "    if number != first_number:\n",
    "      all_same = False\n",
    "    if abs(number - first_number) > 1:\n",
    "      max_diff_one = False\n",
    "\n",
    "  return all_same or max_diff_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0234730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes duplicate matches for sentences in quote classification\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The Dataframe containing the sentence matches\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The Dataframe with duplicates removed\n",
    "    \"\"\"\n",
    "    def find_duplicate_satz(df):\n",
    "        duplicate_indices = {}\n",
    "        for par_satz, indices in df.groupby(['Paragraph', 'Satz']).groups.items():\n",
    "            if len(indices) > 1:\n",
    "                satz_id = str(par_satz[0]) +  \"-\" + str(par_satz[1])\n",
    "                duplicate_indices[satz_id] = list(indices)  # Convert indices to a list\n",
    "        return duplicate_indices\n",
    "\n",
    "    for satz_id, indices in find_duplicate_satz(df.copy()).items():\n",
    "        satz = [int(x) for x in satz_id.split(\"-\")]\n",
    "        if indices[0]-1 in df.index:\n",
    "            check = df[\"Liederbuch\"][indices[0]-1]\n",
    "        else:\n",
    "            check = df[\"Liederbuch\"][indices[-1]+1]\n",
    "        matches = df.query(f\"Paragraph == {satz[0]} and Satz == {satz[1]} and Liederbuch == {check}\").index\n",
    "        if len(matches):\n",
    "            match_index = matches[0]\n",
    "            for i in indices:\n",
    "                if i != match_index:\n",
    "                    df.drop([i], inplace=True)\n",
    "\n",
    "    df.sort_values(by=[\"Ähnlichkeit\"], inplace=True)\n",
    "    df.drop_duplicates(subset=['Paragraph','Satz'], keep='last', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c62e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconsider_match(sent, pages):\n",
    "    highest_match = 0\n",
    "    matches = {}\n",
    "    for page in pages:\n",
    "        with open(f\"source_texts/praxis_pietatis_verses/{page}.json\") as f:\n",
    "            verses = json.load(f)\n",
    "        \n",
    "        for verse in verses:\n",
    "            sim_score = fuzz.ratio(sent, verse)\n",
    "            if sim_score > highest_match:\n",
    "                highest_match = sim_score\n",
    "                matches[sim_score] = [verse, page]\n",
    "\n",
    "    if highest_match > 0:\n",
    "        return [matches[highest_match], highest_match]\n",
    "    else:\n",
    "        return [[\"no match\", 0], 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8178acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_inferred_matches(guessed_hits: pd.DataFrame, sermon: oa.Sermon) -> pd.DataFrame:\n",
    "    for n in range(3):\n",
    "        additional_matches = []\n",
    "        sent_add = lambda x : [x+2,x+3,x+4]\n",
    "        for i in range(0, len(guessed_hits) - 2):\n",
    "            chunk = guessed_hits.iloc[i:i+2]\n",
    "            pages = chunk[\"Liederbuch\"].to_list()\n",
    "            pars = chunk[\"Paragraph\"].to_list()\n",
    "            sents = chunk[\"Satz\"].to_list()\n",
    "            if all(x==pars[0] for x in pars):   # abort if paragraphs change\n",
    "                if sents[1] in sent_add(sents[0]):\n",
    "                    missing_sent = \" \".join(sermon.chunked[pars[0]][sents[0]+1][\"words\"])\n",
    "                    match, sim_score = reconsider_match(missing_sent, [pages[0], pages[1]])\n",
    "                    verse = match[0]\n",
    "                    page = match[1]\n",
    "                    additional_matches.append([missing_sent, \n",
    "                                            pars[0],\n",
    "                                            sents[0]+1, \n",
    "                                            page, \n",
    "                                            verse, \n",
    "                                            float(f\"{sim_score:.2f}\"), \n",
    "                                            False])\n",
    "                    \n",
    "        new_matches = pd.DataFrame(additional_matches, columns=[\"Predigt\", \"Paragraph\", \"Satz\", \n",
    "                                                        \"Liederbuch\", \"Liedvers\", \n",
    "                                                        \"Ähnlichkeit\", \"Dopplung\"])\n",
    "\n",
    "        guessed_hits = pd.concat([guessed_hits, new_matches])\n",
    "        guessed_hits.sort_values([\"Paragraph\", \"Satz\"], ascending=True, inplace=True)\n",
    "        guessed_hits.reset_index(drop=True)\n",
    "    \n",
    "    return guessed_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321c2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_inbetween_matches(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for i in range(0, len(df) - 3):\n",
    "        chunk = df.iloc[i:i+3]\n",
    "        pages = chunk[\"Liederbuch\"].to_list()\n",
    "        pars = chunk[\"Paragraph\"].to_list()\n",
    "        sents = chunk[\"Satz\"].to_list()\n",
    "        if (all(x==pars[0] for x in pars) and not is_equal(pages)):   # abort if paragraphs change or pages are already the same\n",
    "            if pages[0] == pages[2]:\n",
    "                missing_sent = chunk[\"Predigt\"][chunk.index[1]]\n",
    "                print(missing_sent)\n",
    "                match, sim_score = reconsider_match(missing_sent, [pages[0]])\n",
    "                if sim_score > 60:\n",
    "                    verse = match[0]\n",
    "                    page = match[1]\n",
    "                    new_data = [missing_sent, pars[1], sents[1], page, verse, float(f\"{sim_score:.2f}\"), False]\n",
    "                    df.loc[(df['Paragraph'] == pars[1]) & (df[\"Satz\"] == sents[1])] = new_data\n",
    "                    #df.iloc[i] = new_data\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e806f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_page_texts = []\n",
    "#for n in page_nrs:\n",
    "for n in range(41, 1291):\n",
    "    with open(f\"source_texts/praxis_pietatis_verses/{n}.json\") as f:\n",
    "        page = json.load(f)\n",
    "    page_info = {}\n",
    "    page_info[n] = page\n",
    "    relevant_page_texts.append(page_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261b5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sermons_with_most_music.json\", \"r\") as f:\n",
    "    testsermons = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e8078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzziness = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07504d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with E000036\n",
      "Query executed for E100181, but no data found.\n",
      "Query executed for E081074, but no data found.\n",
      "Query executed for E081059, but no data found.\n",
      "Query executed for E081062, but no data found.\n",
      "Query executed for E081071, but no data found.\n",
      "Query executed for E081061, but no data found.\n",
      "Query executed for E081063, but no data found.\n",
      "Query executed for E080981, but no data found.\n",
      "Query executed for E100177, but no data found.\n",
      "Query executed for E100185, but no data found.\n",
      "Query executed for E081069, but no data found.\n",
      "Query executed for E081060, but no data found.\n",
      "Query executed for E081065, but no data found.\n",
      "Query executed for E100180, but no data found.\n",
      "Query executed for E081067, but no data found.\n",
      "Query executed for E080921, but no data found.\n",
      "Query executed for E081072, but no data found.\n",
      "Query executed for E100182, but no data found.\n",
      "Query executed for E081064, but no data found.\n",
      "Query executed for E100158, but no data found.\n",
      "Query executed for E080938, but no data found.\n",
      "Query executed for E091098, but no data found.\n",
      "Query executed for E100183, but no data found.\n",
      "Query executed for E080951, but no data found.\n",
      "Query executed for E100178, but no data found.\n",
      "Query executed for E100173, but no data found.\n",
      "Query executed for E100184, but no data found.\n",
      "Query executed for E100175, but no data found.\n",
      "Query executed for E100176, but no data found.\n",
      "Query executed for E100179, but no data found.\n",
      "Query executed for E081058, but no data found.\n",
      "Query executed for E100174, but no data found.\n",
      "Query executed for E081066, but no data found.\n",
      "Query executed for E081068, but no data found.\n",
      "Query executed for E081073, but no data found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133492/692152878.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  guessed_hits = pd.concat([guessed_hits, new_matches])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jst mein herr jesus christ\n",
      "kein ohr hat je gehört\n",
      "du solt seyn meines hertzenslicht\n",
      "Starting with E000072\n",
      "Starting with E000070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133492/692152878.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  guessed_hits = pd.concat([guessed_hits, new_matches])\n",
      "/tmp/ipykernel_133492/692152878.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  guessed_hits = pd.concat([guessed_hits, new_matches])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dem thu ich mich ergeben\n",
      "Starting with E000055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133492/692152878.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  guessed_hits = pd.concat([guessed_hits, new_matches])\n",
      "/tmp/ipykernel_133492/692152878.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  guessed_hits = pd.concat([guessed_hits, new_matches])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with E000061\n",
      "Query executed for E081026, but no data found.\n",
      "Query executed for E081028, but no data found.\n",
      "Query executed for E100158, but no data found.\n",
      "Query executed for E100160, but no data found.\n",
      "Query executed for E100159, but no data found.\n",
      "Query executed for E081029, but no data found.\n",
      "Query executed for E100157, but no data found.\n",
      "{'type': 'similarity', 'fuzziness': 80, 'date': datetime.datetime(2025, 8, 18, 0, 32, 11, 577579), 'sermons': [{'id': 'E000036', 'agreed_hits': 90, 'divergent_hits': 7, 'new_hits': 25, 'missed_hits': 29, 'avg_certainty': 87.71073770491805, 'precision': 0.7377049180327869, 'recall': 0.7142857142857143, 'f1-score': 0.7258064516129032}, {'id': 'E000072', 'agreed_hits': 29, 'divergent_hits': 5, 'new_hits': 13, 'missed_hits': 21, 'avg_certainty': 85.10021276595744, 'precision': 0.6170212765957447, 'recall': 0.5272727272727272, 'f1-score': 0.5686274509803921}, {'id': 'E000070', 'agreed_hits': 31, 'divergent_hits': 12, 'new_hits': 16, 'missed_hits': 14, 'avg_certainty': 91.39457627118645, 'precision': 0.5254237288135594, 'recall': 0.543859649122807, 'f1-score': 0.5344827586206897}, {'id': 'E000055', 'agreed_hits': 12, 'divergent_hits': 8, 'new_hits': 16, 'missed_hits': 6, 'avg_certainty': 86.30555555555556, 'precision': 0.3333333333333333, 'recall': 0.46153846153846156, 'f1-score': 0.3870967741935484}, {'id': 'E000061', 'agreed_hits': 10, 'divergent_hits': 8, 'new_hits': 28, 'missed_hits': 72, 'avg_certainty': 84.02934782608696, 'precision': 0.21739130434782608, 'recall': 0.1111111111111111, 'f1-score': 0.14705882352941177}], 'overall_precision': 0.4861749122246501, 'overall_recall': 0.47161353266616424, 'overall_f1': 0.47261445178738903, 'overall_certainty': 86.9080860247409}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133492/692152878.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  guessed_hits = pd.concat([guessed_hits, new_matches])\n"
     ]
    }
   ],
   "source": [
    "test_score = {}\n",
    "\n",
    "test_score[\"type\"] = \"similarity\"\n",
    "test_score[\"fuzziness\"] = fuzziness\n",
    "test_score[\"date\"] = datetime.datetime.now()\n",
    "test_score[\"sermons\"] = []\n",
    "\n",
    "for id in testsermons:\n",
    "    print(f\"Starting with {id}\")\n",
    "    sermon = oa.Sermon(id)\n",
    "\n",
    "    # perform classification\n",
    "    hits = []\n",
    "    for i in range(len(sermon.chunked)):                # for each paragraph\n",
    "        for j in range(len(sermon.chunked[i])):         # for each sentence\n",
    "            if \" bibel\" in sermon.chunked[i][j][\"types\"]:\n",
    "                continue\n",
    "            else:\n",
    "                query = \" \".join(sermon.chunked[i][j][\"words\"])\n",
    "                query = re.sub(r'[/.,;:?!]', '', query)\n",
    "                for page in relevant_page_texts:\n",
    "                    for pagenr, verses in page.items():\n",
    "                        for verse in verses:\n",
    "                            sim_score = fuzz.ratio(query, verse)\n",
    "                            if sim_score >= fuzziness:\n",
    "                                hits.append([query, i, j, pagenr, verse, float(f\"{sim_score:.2f}\")])\n",
    "    \n",
    "    guessed_hits = pd.DataFrame(hits, columns=[\"Predigt\", \"Paragraph\", \"Satz\", \"Liederbuch\", \"Liedvers\", \"Ähnlichkeit\"])     # create dataframe\n",
    "    guessed_hits['Dopplung'] = guessed_hits.groupby('Satz')['Satz'].transform(lambda x: x.duplicated())\n",
    "\n",
    "    guessed_hits = remove_duplicates(guessed_hits).reset_index(drop=True)\n",
    "\n",
    "    guessed_hits = add_inferred_matches(guessed_hits, sermon)\n",
    "    guessed_hits = correct_inbetween_matches(guessed_hits)\n",
    "    \n",
    "    guessed_hits.sort_values(\"Satz\", ascending=True, inplace=True)\n",
    "    guessed_hits.reset_index(drop=True)\n",
    "\n",
    "    # create validation set\n",
    "    validation = []\n",
    "    for i in range(len(sermon.chunked)):                # for each paragraph\n",
    "        for j in range(len(sermon.chunked[i])):         # for each sentence\n",
    "            if \" musikwerk\" in sermon.chunked[i][j][\"types\"]:\n",
    "                line = \" \".join(sermon.chunked[i][j][\"words\"])\n",
    "                refs = \", \".join(set(flatten(sermon.chunked[i][j][\"references\"])))\n",
    "                validation.append([line, i, j, refs])\n",
    "\n",
    "    known_hits = pd.DataFrame(validation, columns=[\"Predigt\", \"Paragraph\", \"Satz\", \"Referenz\"])\n",
    "    known_hits = known_hits[known_hits['Referenz'].apply(is_song_in_book)]\n",
    "    known_hits[\"Ref_Seite\"] = known_hits['Referenz'].apply(song_page)\n",
    "\n",
    "    converged_df = pd.merge(known_hits, guessed_hits, on=['Paragraph','Satz'], how='inner')\n",
    "    converged_df[\"in_page_list\"]  = converged_df.apply(lambda row: row['Liederbuch'] in row['Ref_Seite'], axis=1)\n",
    "\n",
    "    # analysis\n",
    "    val_hits = len(known_hits)\n",
    "    \n",
    "    merged_df = pd.merge(guessed_hits, known_hits, on=['Paragraph', 'Satz'], how='left', indicator=True)\n",
    "    hits_not_in_val = len(merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1))\n",
    "    \n",
    "    agreed_hits = converged_df[\"in_page_list\"].value_counts()[True]\n",
    "    divergent_hits = len(converged_df) - agreed_hits\n",
    "    missed_hits = len(known_hits) - (agreed_hits + divergent_hits)\n",
    "    avg_certainty = guessed_hits[\"Ähnlichkeit\"].mean()\n",
    "\n",
    "    precision = agreed_hits / (agreed_hits + divergent_hits + hits_not_in_val)\n",
    "    recall = agreed_hits / val_hits\n",
    "\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"id\"] = id\n",
    "    results[\"agreed_hits\"] = agreed_hits\n",
    "    results[\"divergent_hits\"] = divergent_hits\n",
    "    results[\"new_hits\"] = hits_not_in_val\n",
    "    results[\"missed_hits\"] = missed_hits\n",
    "    results[\"avg_certainty\"] = avg_certainty\n",
    "\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"recall\"] = recall\n",
    "    results[\"f1-score\"] = f1\n",
    "\n",
    "    test_score[\"sermons\"].append(results)\n",
    "\n",
    "all_precision = [x[\"precision\"] for x in test_score[\"sermons\"]]\n",
    "all_recall = [x[\"recall\"] for x in test_score[\"sermons\"]]\n",
    "all_f1 = [x[\"f1-score\"] for x in test_score[\"sermons\"]]\n",
    "all_avg_cert = [x[\"avg_certainty\"] for x in test_score[\"sermons\"]]\n",
    "\n",
    "test_score[\"overall_precision\"] = statistics.mean(all_precision)\n",
    "test_score[\"overall_recall\"] = statistics.mean(all_recall)\n",
    "test_score[\"overall_f1\"] = statistics.mean(all_f1)\n",
    "test_score[\"overall_certainty\"] = statistics.mean(all_avg_cert)\n",
    "\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4772436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type', 'similarity')\n",
      "('fuzziness', 80)\n",
      "('date', datetime.datetime(2025, 8, 18, 0, 32, 11, 577579))\n",
      "('sermons', [{'id': 'E000036', 'agreed_hits': 90, 'divergent_hits': 7, 'new_hits': 25, 'missed_hits': 29, 'avg_certainty': 87.71073770491805, 'precision': 0.7377049180327869, 'recall': 0.7142857142857143, 'f1-score': 0.7258064516129032}, {'id': 'E000072', 'agreed_hits': 29, 'divergent_hits': 5, 'new_hits': 13, 'missed_hits': 21, 'avg_certainty': 85.10021276595744, 'precision': 0.6170212765957447, 'recall': 0.5272727272727272, 'f1-score': 0.5686274509803921}, {'id': 'E000070', 'agreed_hits': 31, 'divergent_hits': 12, 'new_hits': 16, 'missed_hits': 14, 'avg_certainty': 91.39457627118645, 'precision': 0.5254237288135594, 'recall': 0.543859649122807, 'f1-score': 0.5344827586206897}, {'id': 'E000055', 'agreed_hits': 12, 'divergent_hits': 8, 'new_hits': 16, 'missed_hits': 6, 'avg_certainty': 86.30555555555556, 'precision': 0.3333333333333333, 'recall': 0.46153846153846156, 'f1-score': 0.3870967741935484}, {'id': 'E000061', 'agreed_hits': 10, 'divergent_hits': 8, 'new_hits': 28, 'missed_hits': 72, 'avg_certainty': 84.02934782608696, 'precision': 0.21739130434782608, 'recall': 0.1111111111111111, 'f1-score': 0.14705882352941177}])\n",
      "('overall_precision', 0.4861749122246501)\n",
      "('overall_recall', 0.47161353266616424)\n",
      "('overall_f1', 0.47261445178738903)\n",
      "('overall_certainty', 86.9080860247409)\n"
     ]
    }
   ],
   "source": [
    "for key, value in test_score.items():\n",
    "    print((key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a407ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-08-18T00:32:11.577579'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score[\"date\"] = test_score[\"date\"].isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd010b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_results.json\", \"r\") as f:\n",
    "    test_results = json.load(f)\n",
    "\n",
    "test_results.append(test_score)\n",
    "\n",
    "with open(\"test_results.json\", \"w\") as f:\n",
    "    json.dump(test_results, f, ensure_ascii=False, cls=NumpyEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_orgelpredigt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
